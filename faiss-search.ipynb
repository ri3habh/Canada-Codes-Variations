{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to Implement FAISS for Retrieval-Based Search\n",
    "Instead of Azure Semantic Search, we’ll:\n",
    "\n",
    "Embed the National Code sentences using GIST-Embedding-v0.\n",
    "Index these embeddings in FAISS.\n",
    "Embed the Provincial Code sentences and use FAISS to retrieve the most similar National Code sentence.\n",
    "Classify the differences (Exact Match, Modified, New Addition, Omitted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.4.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.48.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sharmari\\appdata\\roaming\\python\\python312\\site-packages (from faiss-cpu) (24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sharmari\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sharmari\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sharmari\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\sharmari\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers faiss-cpu pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sharmari\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_path = \"./GIST_Local\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦅 National Code indexed successfully in FAISS! 🦅\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load National Code dataset\n",
    "national_df = pd.read_csv(\"national_dataset.csv\")\n",
    "\n",
    "# Extract relevant columns\n",
    "national_metadata = national_df[\n",
    "    [\"Division\", \"National Section ID\", \"Section Number\", \"National Subsection ID\",\n",
    "     \"Subsection Number\", \"National Article Title\", \"National Sentence Number\", \"National Sentence Text\"]\n",
    "]\n",
    "\n",
    "# Drop missing sentences\n",
    "national_metadata = national_metadata.dropna(subset=[\"National Sentence Text\"])\n",
    "\n",
    "# Create FAISS index\n",
    "embedding_dim = 768  # GIST model outputs 768-dimensional vectors\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "def process_and_index_embeddings(text_list, batch_size=4):\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch = text_list[i:i+batch_size]\n",
    "\n",
    "        # Tokenization\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy().astype(np.float16)  # Use float16\n",
    "\n",
    "        # Add directly to FAISS index (instead of keeping in memory)\n",
    "        index.add(embeddings)\n",
    "\n",
    "national_sentences = national_metadata[\"National Sentence Text\"].tolist()\n",
    "process_and_index_embeddings(national_sentences, batch_size=4)  # Smaller batch size\n",
    "\n",
    "# Save FAISS index and metadata\n",
    "faiss.write_index(index, \"national_code_index.faiss\")\n",
    "national_metadata.to_csv(\"national_metadata.csv\", index=False)\n",
    "\n",
    "print(\"🦅 National Code indexed successfully in FAISS! 🦅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐐 Optimized: Similar sentences retrieved with metadata! 🐐\n"
     ]
    }
   ],
   "source": [
    "provincial_df = pd.read_csv(\"alberta_dataset.csv\")\n",
    "\n",
    "provincial_metadata = provincial_df[\n",
    "    [\"Division\", \"P/T Section ID\", \"Section Number\", \"P/T Subsection ID\",\n",
    "     \"Subsection Number\", \"P/T Article Title\", \"P/T Sentence Number\", \"P/T Sentence Text\"]\n",
    "]\n",
    "\n",
    "provincial_metadata = provincial_metadata.dropna(subset=[\"P/T Sentence Text\"])\n",
    "\n",
    "def get_embeddings(text_list, batch_size=4):\n",
    "    embeddings = []\n",
    "\n",
    "    for i in range(0, len(text_list), batch_size):\n",
    "        batch = text_list[i:i+batch_size]\n",
    "\n",
    "        # Tokenization\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy().astype(np.float16)  # Use float16\n",
    "\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    return np.vstack(embeddings)  # Merge all batches\n",
    "\n",
    "provincial_sentences = provincial_metadata[\"P/T Sentence Text\"].tolist()\n",
    "provincial_embeddings = get_embeddings(provincial_sentences, batch_size=4)  # Smaller batch size\n",
    "\n",
    "index = faiss.read_index(\"national_code_index.faiss\")\n",
    "national_metadata = pd.read_csv(\"national_metadata.csv\")\n",
    "\n",
    "D, I = index.search(provincial_embeddings, 1)  # 1 nearest neighbor\n",
    "\n",
    "matched_sentences = []\n",
    "for i, (pt_sentence, best_match_idx, score) in enumerate(zip(provincial_metadata[\"P/T Sentence Text\"], I[:, 0], D[:, 0])):\n",
    "    matched_sentences.append([\n",
    "        *provincial_metadata.iloc[i].tolist(),  # Provincial metadata\n",
    "        *national_metadata.iloc[best_match_idx].tolist(),  # National metadata\n",
    "        score\n",
    "    ])\n",
    "\n",
    "columns = list(provincial_metadata.columns) + list(national_metadata.columns) + [\"Distance\"]\n",
    "matched_df = pd.DataFrame(matched_sentences, columns=columns)\n",
    "matched_df.to_csv(\"retrieved_similar_sentences_with_metadata.csv\", index=False)\n",
    "\n",
    "print(\"🐐 Optimized: Similar sentences retrieved with metadata! 🐐\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐍 Optimized: Changes stored in retrieved_similar_sentences_with_changes.csv! 🐍\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "\n",
    "matched_df = pd.read_csv(\"retrieved_similar_sentences_with_metadata.csv\")\n",
    "\n",
    "def classify_change(nat_text, pt_text):\n",
    "    if pd.isna(nat_text) or pd.isna(pt_text):\n",
    "        return \"Missing Data\"\n",
    "    \n",
    "    if nat_text.strip() == pt_text.strip():\n",
    "        return \"Exact Match\"\n",
    "\n",
    "    similarity = SequenceMatcher(None, nat_text, pt_text).ratio()\n",
    "\n",
    "    if similarity > 0.8:\n",
    "        return \"Modified\"\n",
    "    elif pt_text not in nat_text:\n",
    "        return \"New Addition\"\n",
    "    else:\n",
    "        return \"Omitted\"\n",
    "\n",
    "matched_df[\"Change Type\"] = [\n",
    "    classify_change(nat, pt) for nat, pt in zip(matched_df[\"National Sentence Text\"], matched_df[\"P/T Sentence Text\"])\n",
    "]\n",
    "\n",
    "matched_df.to_csv(\"retrieved_similar_sentences_with_changes.csv\", index=False)\n",
    "\n",
    "print(\"🐍 Optimized: Changes stored in retrieved_similar_sentences_with_changes.csv! 🐍\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
